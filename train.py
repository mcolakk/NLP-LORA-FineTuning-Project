# train.py
# ==============================================================================
# NLP PROJESÄ° - LORA FINE-TUNING EÄÄ°TÄ°M KODU (V4 CUSTOM)
# Model: Qwen/Qwen2.5-Coder-1.5B-Instruct
# Ayarlar: Rank 16, Alpha 32, Dropout 0.1, LR 2e-5, Weight Decay 0.1
# ==============================================================================

import os
import torch
import gc
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling,
    EarlyStoppingCallback
)
from peft import LoraConfig, get_peft_model, TaskType
from datasets import load_dataset

# ==========================================
# âš™ï¸ HÄ°PERPARAMETRELER (V4 AYARLARI)
# ==========================================
MODEL_ID = "Qwen/Qwen2.5-Coder-1.5B-Instruct"
OUTPUT_DIR_BASE = "./results"  # GitHub iÃ§in yerel klasÃ¶r ayarlandÄ±

# EÄŸitim AyarlarÄ± (V4 - Stability & Anti-Overfit)
NUM_EPOCHS = 3
LEARNING_RATE = 2e-5     # DÃ¼ÅŸÃ¼k Ã¶ÄŸrenme hÄ±zÄ±
WEIGHT_DECAY = 0.1       # AÄŸÄ±rlÄ±k Ã§Ã¼rÃ¼mesi (Overfitting Ã¶nleyici)
BATCH_SIZE = 4
GRAD_ACCUMULATION = 4
CONTEXT_LENGTH = 1024
PATIENCE = 3             # Early Stopping sabrÄ±

# LoRA AyarlarÄ± (V4)
LORA_R = 16              # Rank
LORA_ALPHA = 32          # Alpha
LORA_DROPOUT = 0.1       # Dropout artÄ±rÄ±ldÄ±

SYSTEM_PROMPT = "You are an expert Python programmer. Please read the problem carefully before writing any Python code."

# HafÄ±za YÃ¶netimi
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

# ==========================================
# ğŸ› ï¸ EÄÄ°TÄ°M FONKSÄ°YONU
# ==========================================
def train_model_v4(dataset_name, hf_dataset_path):
    # Temizlik
    torch.cuda.empty_cache()
    gc.collect()

    output_dir = f"{OUTPUT_DIR_BASE}/{dataset_name}_Model_V4_Custom"
    print(f"\n{'='*50}")
    print(f"ğŸš€ {dataset_name} EÄÄ°TÄ°MÄ° BAÅLIYOR (V4)...")
    print(f"ğŸ“‚ KayÄ±t Yeri: {output_dir}")

    # 1. Dataset YÃ¼kleme ve AyrÄ±ÅŸtÄ±rma
    print("ğŸ“¥ Veri seti yÃ¼kleniyor...")
    try:
        full_dataset = load_dataset(hf_dataset_path, split="train")

        # Senin kodundaki Ã¶zel split mantÄ±ÄŸÄ±
        if "split" in full_dataset.column_names:
            print("âœ… 'split' sÃ¼tunu bulundu. AyrÄ±ÅŸtÄ±rÄ±lÄ±yor...")
            train_dataset = full_dataset.filter(lambda x: x["split"] == "train")
            eval_dataset = full_dataset.filter(lambda x: x["split"] == "test")

            if len(eval_dataset) == 0:
                print("âš ï¸ Test etiketi boÅŸ, 'valid' etiketi deneniyor...")
                eval_dataset = full_dataset.filter(lambda x: x["split"] == "valid")
        else:
            # Split sÃ¼tunu yoksa otomatik ayÄ±r
            print("âš ï¸ 'split' sÃ¼tunu yok. Otomatik %10 validation ayrÄ±lÄ±yor.")
            split_data = full_dataset.train_test_split(test_size=0.1)
            train_dataset = split_data["train"]
            eval_dataset = split_data["test"]

        print(f"âœ… HazÄ±r: Train ({len(train_dataset)}) | Val ({len(eval_dataset)})")

    except Exception as e:
        print(f"âŒ Veri seti yÃ¼kleme hatasÄ±: {e}")
        return

    # 2. Model & Tokenizer YÃ¼kleme
    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)
    tokenizer.pad_token = tokenizer.eos_token

    # Model yÃ¼kleme (4-bit quantization config eklenebilir, burada standart yÃ¼kleme var)
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_ID,
        torch_dtype=torch.bfloat16,
        attn_implementation="sdpa",
        device_map="auto",
        use_cache=False
    )

    model.gradient_checkpointing_enable()
    model.enable_input_require_grads()

    # 3. LoRA YapÄ±landÄ±rmasÄ±
    peft_config = LoraConfig(
        r=LORA_R,
        lora_alpha=LORA_ALPHA,
        lora_dropout=LORA_DROPOUT,
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
        task_type=TaskType.CAUSAL_LM,
        bias="none"
    )
    model = get_peft_model(model, peft_config)
    model.print_trainable_parameters()

    # 4. Veri Formatlama (Chat Template)
    def format_chat(sample):
        messages = [
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": sample["input"]},
            {"role": "assistant", "content": sample["solution"]}
        ]
        text = tokenizer.apply_chat_template(messages, tokenize=False)
        tokenized = tokenizer(text, truncation=True, max_length=CONTEXT_LENGTH, padding="max_length")
        tokenized["labels"] = tokenized["input_ids"].copy()
        return tokenized

    print("ğŸ”„ Veriler iÅŸleniyor (Tokenization)...")
    train_dataset = train_dataset.map(format_chat, remove_columns=train_dataset.column_names)
    eval_dataset = eval_dataset.map(format_chat, remove_columns=eval_dataset.column_names)

    # 5. Training Arguments
    training_args = TrainingArguments(
        output_dir=output_dir,
        per_device_train_batch_size=BATCH_SIZE,
        per_device_eval_batch_size=BATCH_SIZE,
        gradient_accumulation_steps=GRAD_ACCUMULATION,
        learning_rate=LEARNING_RATE,
        weight_decay=WEIGHT_DECAY,
        num_train_epochs=NUM_EPOCHS,
        eval_strategy="steps",
        eval_steps=100,
        save_strategy="steps",
        save_steps=100,
        logging_steps=20,
        gradient_checkpointing=True,
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        greater_is_better=False,
        bf16=True, fp16=False, 
        optim="adamw_torch_fused", 
        report_to="none"
    )

    # 6. Trainer BaÅŸlatma
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),
        callbacks=[EarlyStoppingCallback(early_stopping_patience=PATIENCE)]
    )

    # EÄŸitimi BaÅŸlat
    trainer.train()

    # Modeli Kaydet
    final_path = f"{output_dir}/final_model"
    model.save_pretrained(final_path)
    tokenizer.save_pretrained(final_path)
    print(f"âœ… {dataset_name} EÄŸitimi TamamlandÄ±! KayÄ±t: {final_path}")

    # Bellek TemizliÄŸi
    del model
    del trainer
    torch.cuda.empty_cache()
    gc.collect()

# ==========================================
# â–¶ï¸ ANA Ã‡ALIÅTIRMA BLOÄU
# ==========================================
if __name__ == "__main__":
    # Not: GitHub'da Ã§alÄ±ÅŸtÄ±rmak isteyenler iÃ§in dataset isimleri
    print("EÄŸitim Scripti BaÅŸlatÄ±lÄ±yor...")
    
    # 1. DEEP EÄŸitimi
    train_model_v4("DEEP", "Naholav/CodeGen-Deep-5K")
    
    # 2. DIVERSE EÄŸitimi
    train_model_v4("DIVERSE", "Naholav/CodeGen-Diverse-5K")